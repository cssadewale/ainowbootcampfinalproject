# -*- coding: utf-8 -*-
"""Adewale AI now bootcamp Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2O7I9-GzV5nJ3onSzGmz_Nr4gHa_kNT

#**CAPSTONE PROJECT**    


**About the project**        

You have been appointed as the Lead Data Analyst to build a predictive model to determine if a building will have an insurance claim during a certain period or not. You will have to predict the probability of having at least one claim over the insured period of the building. The model will be based on the building characteristics. The target variable, Claim, is a:

* 1 if the building has at least a claim over the insured period.      
* 0 if the building doesn’t have a claim over the insured period.    

You have the data description and the training data too

Ensure you do all the necessary data preprocessing to improve your result, and evaluate too    

Also, your work should be added as a project to your GitHub repository. The link to the repository that has your notebook(s) is what is expected to be submitted.       


A good project contains         
1. A good data cleaning and preprocessing
2. ⁠A detailed and explained set of insights through Exploratory Data Analysis
3. ⁠A well-experimented preprocessing for modelling
4. ⁠Implement more than 1 model
5. ⁠Evaluate the models

#**STEP 1: BASIC DATA ANALYSIS (OR BASIC DATA EXPLORATION)**
This is to understand the dataframe, including initial data loading (or collection), inspection, and cleaning.

##**1. DATA COLLECTION (DATA LOADING)**

This involves importing the neccesary libraries for data importation, manipulation, visualization, preprocessing, model building and evaluation

###**a.Installing and Importing the necessary libraries**

**Installing libraries for generating report**
"""

!pip install ydata_profiling
from ydata_profiling import ProfileReport # for generating a report

"""**importing other libraries**"""

import warnings # for importing warnings module
warnings.filterwarnings('ignore')  # to set filter to ignore warning
import gdown  # for importing dataframe from google drive link
import pandas as pd # for data manipulation
import numpy as np  # for numerical operation
import matplotlib.pyplot as plt  # for data visualization
import seaborn as sns  # for data visualization
plt.style.use('ggplot')  # for setting the style of the plot
sns.set_theme(style="darkgrid")  # for setting the theme of the plot
print ("Libraries imported successfully")

"""###**b. Importing the DataFrame**

**Feature or Variable Description Dataframe**
"""

url = "https://drive.google.com/uc?export=download&id=1suexiz-Q6_2MCkifBqhvnk9aK48UlRBs"
gdown.download(url, "data_description.csv", quiet=True)

# Read the CSV file into a DataFrame
df_desc = pd.read_csv("data_description.csv")
print ("Dataframe imported successfully")

"""**Actual DataFrame**"""

url = "https://drive.google.com/uc?export=download&id=10mm9VnlmkfR1-hlhjLKvgsMu9i7Vlwr1"
gdown.download(url, "data.csv", quiet=True)

# Read the csv file into a dataframe
df = pd.read_csv("data.csv")
print ("Dataframe imported successfully")

"""##**2. DATA INSPECTION**
This helps us to understand the data. Here, we do basic data exploratioon.

**Display options**
"""

# Set display options to show all rows and columns

# display all rows without truncation
pd.set_option('display.max_rows', None)

# display all coluumns without truncation
pd.set_option('display.max_columns', None)

# display maximum column width, preventing text in wide column from being truncated
pd.set_option('display.max_colwidth', None)

# prevent df from expanding to multiple lines if it is too wide,
# keeping it in a single block if possible
pd.set_option('display.expand_frame_repr', False)


print('Display options set')

"""###**A. The Overview of The DataFrame**

**Viewing Features or Variables Description**
"""

df_desc

"""**Viewing some rows**"""

df.sample(30)

"""*   **Missing/Inconsistent Values:**
    *   The `Date_of_Occupancy` and `Building_Dimension` columns appear to have missing values represented as `NaN`.
    *   The `NumberOfWindows` column contains `.` which likely represents missing or unknown values. This indicates it might not be a purely numerical column and will require careful handling during data preprocessing (e.g., converting `.` to `NaN` and then to a numeric type, or imputing).
    *   The `Geo_Code` column also contains non-standard, alphanumeric values (e.g., `'2A004'`), suggesting it might be a string type or a mixed-type column.
*   **Data Type Indications:**
    *   `Insured_Period` shows float values, some of which are fractional, implying precise duration measurements.
    *   `Residential`, `Building_Painted`, `Building_Fenced`, `Garden`, `Settlement`, and `Building_Type` are clearly categorical features with distinct string or integer representations (e.g., 'V', 'N', 'O', 'U', 'R', 1, 2, 3, 4).
    *   `YearOfObservation` and `Date_of_Occupancy` (year) appear to be numerical, but `Date_of_Occupancy` has missing values.
*   **Target Variable:**
    *   The `Claim` column, which is the target variable, contains `0` and `1`, indicating a binary classification problem (presence or absence of a claim).

**III. The Shape of the DataFrame**
"""

df.shape

"""The output of `df.shape` is `(7160, 14)`. This indicates:

*   **7160 Rows**: The dataset contains 7160 individual observations or records.
*   **14 Columns**: There are 14 features or variables in the dataset.

This tells us the exact dimensions of our dataset.

###**B. The Concise Summary of the DataFrame**       
`df.info()` is a method of the pandas DataFrame. This provides a concise summary of the DataFrame.
"""

df.info()

"""Here are the key insights from the `df.info()` output:

*   **Total Entries:** There are 7160 entries (rows) in the DataFrame, indexed from 0 to 7159.
*   **Total Columns:** There are 14 columns.
*   **Data Types:**
    *   `float64`: 3 columns (`Insured_Period`, `Building Dimension`, `Date_of_Occupancy`).
    *   `int64`: 4 columns (`YearOfObservation`, `Residential`, `Building_Type`, `Claim`).
    *   `object` (likely strings/mixed types): 7 columns (`Customer Id`, `Building_Painted`, `Building_Fenced`, `Garden`, `Settlement`, `NumberOfWindows`, `Geo_Code`).
*   **Missing Values (Non-Null Counts):**
    *   **`Garden`**: 7153 non-null out of 7160 (7 missing values).
    *   **`Building Dimension`**: 7054 non-null out of 7160 (106 missing values).
    *   **`Date_of_Occupancy`**: 6652 non-null out of 7160 (508 missing values).
    *   **`Geo_Code`**: 7058 non-null out of 7160 (102 missing values).
    *   All other columns have 7160 non-null values, meaning they are complete.
*   **Potential Issues:**
    *   The `NumberOfWindows` column is of `object` dtype but based on the sample, it contains numerical values and '.' characters. This will need cleaning and conversion to a numeric type.
    *   The `Geo_Code` column is also `object` dtype and contains mixed alphanumeric values, as seen in the `df.sample()` output (e.g., '2A004'). This might require specific encoding or treatment if used in modeling.
    *   Several columns have missing values, which will need to be addressed (imputation, removal, etc.) during data preprocessing.

### **C. Descriptive Statistics of the DataFrame**
**(Also called Summary Statistics)**   
This provides a summary of the central tendency, dispersion, and shape of a dataset's distribution, excluding `NaN` values, for numerical columns.
"""

df.describe()

"""Here are the key insights from the `df.describe()` output, focusing on numerical columns:

*   **YearOfObservation**: The data covers observations from 2012 to 2016, with a mean year of 2013.67. This indicates a relatively recent timeframe for the insurance policies.
*   **Insured_Period**: The insurance periods range from 0 to 1 year. The mean of 0.91 suggests that most policies are full-year, but there are also shorter durations, which need to be accounted for in analysis.
*   **Residential**: This is a binary variable (0 for non-residential, 1 for residential). The mean of approximately 0.305 indicates that about 30.5% of the buildings in the dataset are residential.
*   **Building Dimension**: Building sizes vary significantly, from a minimum of 1 m² to a maximum of 20940 m². The mean is around 1883.73 m², but the large standard deviation of 2278.16 m² points to a wide spread and potentially some very large outliers. The count (7054) confirms there are missing values in this column.
*   **Building_Type**: As a categorical variable treated numerically, its values range from 1 to 4. The mean of 2.19 reflects the distribution across these types, indicating no single type dominates overwhelmingly.
*   **Date_of_Occupancy**: This column shows a broad range of occupancy years, from 1545 to 2016, with an average around 1964.46. The minimum year (1545) is extremely early and might indicate a data entry error or an interesting historical building. Further investigation or outlier handling might be needed. The count (6652) confirms a significant number of missing values (508).
*   **Claim**: This is the target variable (0 for no claim, 1 for at least one claim). The mean of 0.228 indicates that approximately 22.8% of the buildings in the dataset had at least one insurance claim. This suggests an imbalanced dataset, which is an important consideration for model building, as it might require techniques like oversampling, undersampling, or using appropriate evaluation metrics.

##**3. DATA CLEANING**        
Data cleaning is a crucial step in the data science workflow (or data analysis process) where you identify and correct errors, inconsistencies, and inaccuracies in your dataset.

###**a.Cleaning Column Names:**
Here, we can do the following;
* Changing column names to lower case.
* Removing leading or trailing whitespaces.
* Replacing spaces with underscores.
* Renaming some columns if necessary.
* Dropping some columns if necessary.
"""

df.columns = df.columns.str.lower()  # change column names to lowercase
df.columns = df.columns.str.strip()  # remove leading/trailing whitespace
df.columns = df.columns.str.replace(" ", "_") # replacing spaces with underscores

# all this can be done using using a single line of code
#df.columns = df.columns.str.lower().str.strip().str.replace(" ", "_")

df.columns

"""We are going to rename some column name"""

df.rename(columns={
    "yearofobservation": "year_of_observation",
    "numberofwindows": "number_of_windows"
}, inplace=True)
print('Column Rename Done')

"""Now,  let's check"""

df.columns.to_list()

"""**Dropping Irrelevant Columns**

The `customer_id` and `geo_code` columns will be removed from the DataFrame as they are identifiers and not predictive features for the model.

"""

df.drop(columns=['customer_id', 'geo_code'], axis=1, inplace=True)
print("Columns 'customer_id' and 'geo_code' dropped successfully.")
display(df.columns.to_list())

"""The `customer_id` and `geo_code` columns were dropped.
`customer_id` was removed because it is a unique identifier for ech policy holder and does not contain predictive information for the model. Including it would add noise and could lead to issues if treated as a feature.

`geo_code` was removed because it is too granular and complex to use directly without significant feature engineering, simplifying the model.

###**b. Data Type Verification and Correction**           
This involves confirming that each column has the correct data type (e.g., numerical columns are `int` or `float`, categorical columns are `object` or `category`, and date columns are `datetime`).
"""

# Check the data types of all columns
print("Data types of each column:")
print(df.dtypes)

print("\nData type verification complete.")

"""We will change the data types of the variables (year_of_observation and date_of_occupancy) to datetime data types"""

df['year_of_observation'] = pd.to_datetime(df['year_of_observation'], format='%Y', errors='coerce')
df['date_of_occupancy'] = pd.to_datetime(df['date_of_occupancy'], format='%Y', errors='coerce')
df['residential'] = df['residential'].astype('object')
df['building_type'] = df['building_type'].astype('object')

# verify if the data types has been changed
df.dtypes

"""###**c. Unique Values Inspection**   
This includes:
* Unique Value Counts
* The List of Unique Values in Each Column
* The Frequency of Each Unique Value in Each Column
* Conversion Of Strange Unique Value to Nan
"""

for col in df.columns:
  print("Name of Column:", col)
  print()
  print(f"Non-null count in {col}:", df[col].count())
  print()
  print(f"Number of Values in {col}:", df[col].nunique())
  print()
  print(f"Each unique value in {col}:", df[col].unique())
  print()
  print("*************************"*10)
  print()

"""Here are the key insights from the unique values inspection:

*   **`year_of_observation`**: Contains 5 unique years (2012-2016) and has no missing values, as expected for a `datetime` type.
*   **`insured_period`**: Shows a wide range of fractional and whole numbers (401 unique values) between 0 and 1, indicating varying policy durations. No missing values.
*   **Categorical Features (`residential`, `building_painted`, `building_fenced`, `settlement`, `building_type`)**: These columns appear to be clean with distinct categories and no missing values, aligning with their categorical nature.
*   **`garden`**: Contains 'V', 'O', and `nan` values, indicating 7 missing entries that need to be addressed.
*   **`building_dimension`**: Has a large number of unique values (2043) and 106 missing values, suggesting it's a continuous numerical feature with some missing data.
*   **`date_of_occupancy`**: Now correctly parsed as `datetime` but shows `NaT` for 516 entries (7160 - 6644 = 516), indicating that some original values could not be converted to a valid date. The minimum year (e.g., 1545 seen in `describe()`) is still present and needs further review as a potential outlier or data entry error.
*   **`number_of_windows`**: This column is currently an `object` type with 11 unique values including `'.'` and `'>=10'`. This confirms it needs significant cleaning to be converted into a usable numerical format, as `'.'` likely represents missing values and `'>=10'` is a categorical representation of a range.
*   **`claim`**: The target variable has only two unique values (0 and 1), confirming its binary nature and cleanliness.

From inspection, it seems that there is a strange unique value (".") in the variable, number_of_window and the strange unique value is surrounded by whitespaces.  We will replace this by 0.
"""

# Replace any string that is just a "." surrounded by any amount of whitespace
df['number_of_windows'] = df['number_of_windows'].replace(r'^\s*\.\s*$', "0", regex=True)


# let's check the unique value
df['number_of_windows'].unique()

"""In the column, `number of window`, we will Convert the string value '>=10' to a suitable numerical representation (e.g., 10 or 10.0 for consistency) and then change the entire column's data type to integer or float. This ensures it can be used in numerical analysis.

"""

df['number_of_windows'] = df['number_of_windows'].replace({'>=10': '10'})
df['number_of_windows'] = df['number_of_windows'].astype(int)

print("Unique values in 'number_of_windows' after cleaning:", df['number_of_windows'].unique())
print("Data type of 'number_of_windows' after conversion:", df['number_of_windows'].dtype)

"""###**d. Missing Values Check and Handling**      

This involves:
* Identifying Columns with Missing Values
* Percentages of Missing Values in each Column
* Handling The Missing Values
"""

# Count missing values per column
missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_report = pd.DataFrame({
    'Missing Values': missing_values,
    'Missing (%)': missing_percentage.round(2)
})

missing_report = missing_report.sort_values(by='Missing (%)', ascending=False)
display(missing_report)

"""Columns with missing Values"""

missing_report[missing_report['Missing Values'] > 0]

"""**Handling the Missing Values**"""

# handling missing values for 'garden'
df['garden'].fillna(df['garden'].mode()[0], inplace=True)

# handling missing values for 'date_of_occupancy
df['date_of_occupancy'].fillna(df['date_of_occupancy'].mode()[0], inplace=True)

# handling missing values for 'building_dimension'
df['building_dimension'].fillna(df['building_dimension'].mode()[0], inplace=True)

# check to confirm if missing values has been handled
print("After handling missing values:\n", df.isna().sum())

"""###**e. Duplicate Values Check**"""

# Check for duplicate row
duplicate_row = df.duplicated().sum()
print("Number of duplicate rows:", duplicate_row)

"""The above result shows that there are 146 duplicated observation. We will drop these duplicated observation."""

if duplicate_row > 0:
    df.drop_duplicates(inplace=True)
    print("Duplicate rows have been removed.")
else:
    print("No duplicate rows found.")

"""Let's now check the new shape of the dataframe."""

df.shape

"""Now, the dataframe contain 7014 rows and 12 columns

##**4. Feature Engineering (Creating New Feature)**

We will create a new numerical feature 'building_age' by calculating the difference between the `year_of_observation` and the `year_of_occupancy`.
"""

df['observation_year'] = df['year_of_observation'].dt.year
df['occupancy_year'] = df['date_of_occupancy'].dt.year
df['building_age'] = df['observation_year'] - df['occupancy_year']
df.head()

"""##**5. SKEWNESS ANALYSIS FOR NUMERICAL COLUMNS**"""

num_df = df.select_dtypes(include=['int64', 'float64'])
numerical_features = num_df.columns
print("Skewness of numerical features:")
display(df[numerical_features].skew())

"""Here's an analysis of the skewness for the numerical features in the dataset:

*   **`insured_period` (Skewness: -2.718137):**
    *   This feature is **strongly negatively (left) skewed**. This means that the tail of the distribution extends towards the left, and the majority of the `insured_period` values are concentrated towards the higher end of the range. Most insurance policies are for longer durations, possibly full-year, with fewer policies for very short periods. This could imply that a significant portion of buildings are insured for almost the maximum duration.

*   **`building_dimension` (Skewness: 3.116899):**
    *   This feature is **strongly positively (right) skewed**. The tail of the distribution extends significantly towards the right, indicating that most buildings have smaller dimensions, but there are a few buildings with very large dimensions that pull the mean to the right. This suggests the presence of outliers or a natural distribution where small to medium-sized buildings are common, and extremely large buildings are rare.

*   **`number_of_windows` (Skewness: 0.796970):**
    *   This feature shows **moderate positive (right) skewness**. The distribution has a tail extending to the right, meaning that most buildings have a lower number of windows, while a smaller number of buildings have a considerably higher count of windows. This is a common pattern for counts data where lower values are more frequent.

*   **`claim` (Skewness: 1.271269):**
    *   As a binary target variable (0 or 1), a positive skewness value here is expected. It indicates that the count of '0' (no claim) is significantly higher than the count of '1' (at least one claim). This confirms the observation from `df.describe()` that only about 22.8% of buildings had a claim, highlighting an **imbalanced dataset**. This imbalance is a crucial insight for model building, as it often requires specific handling techniques (e.g., resampling, appropriate evaluation metrics) to prevent the model from being biased towards the majority class.

##**6. OUTLIER DETECTION (Using IQR Method)**
We will use the Interquartile Range (IQR) method to identify potential outliers in the numerical columns.
"""

print("Identifying potential outliers using the IQR method:")
num_df = df.select_dtypes(include=['int64', 'float64'])
for col in num_df:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    print(f"\n--- Potential Outliers in '{col}' ---")
    print(f"  IQR: {IQR:.2f}")
    print(f"  Lower Bound (Q1 - 1.5*IQR): {lower_bound:.2f}")
    print(f"  Upper Bound (Q3 + 1.5*IQR): {upper_bound:.2f}")
    print(f"  Number of potential outliers: {len(outliers)}")
    # Display a few of the outlier rows if there are any
    if not outliers.empty:
        print("  Sample Outlier Rows:")
        display(outliers.head())
    else:
        print("  No potential outliers detected by IQR method in this column.")

print("\nOutlier detection complete.")

"""The IQR method has identified potential outliers in several numerical columns:

1.  **`insured_period` (1233 potential outliers)**:
    *   `IQR: 0.00`, `Lower Bound: 0.99`, `Upper Bound: 1.00`.
    *   The `insured_period` column shows a very high number of outliers (1233 out of 7014 entries, approximately 17.6%). The IQR is 0, which means that 50% of the data (between Q1 and Q3) has the same value. Given the min/max values and previous insights, this likely indicates that the vast majority of policies are for a full year (1.0), and anything less than that is considered an outlier by this specific IQR calculation. This isn't necessarily 'bad' data but highlights the distribution's heavy concentration at 1.0. Depending on the modeling approach, these shorter periods might need special handling or a different interpretation rather than outright removal.

2.  **`building_dimension` (533 potential outliers)**:
    *   `IQR: 1777.75`, `Lower Bound: -2151.62`, `Upper Bound: 4959.38`.
    *   There are 533 potential outliers, representing about 7.6% of the data. As noted in the skewness analysis, this column is highly positively skewed, suggesting the presence of buildings with exceptionally large dimensions. These outliers are above the `upper_bound` of 4959.38. These large values could significantly influence models, and further investigation is needed to determine if they are valid extreme cases or data entry errors. Techniques like Winsorization or logarithmic transformations might be considered.

3.  **`number_of_windows` (0 potential outliers)**:
    *   `IQR: 4.00`, `Lower Bound: -6.00`, `Upper Bound: 10.00`.
    *   No potential outliers were detected by the IQR method. This suggests that while the distribution is moderately skewed, the values generally fall within the expected range, and there are no extreme values that lie significantly outside the interquartile range.

4.  **`claim` (1626 potential outliers)**:
    *   `IQR: 0.00`, `Lower Bound: 0.00`, `Upper Bound: 0.00`.
    *   Similar to `insured_period`, the `claim` column (our target variable) shows a large number of 'outliers' (1626 entries, approximately 23.2%). This is because `claim` is a binary variable (0 or 1), and the IQR method identifies the minority class (Claim=1) as outliers when the majority class (Claim=0) dominates. This reinforces the observation of **class imbalance**, which is a critical factor to address during model training (e.g., using techniques like SMOTE, class weighting, or appropriate evaluation metrics like precision, recall, or F1-score rather than just accuracy).

#**STEP 2. EXPLORATORY DATA ANALYSIS**     

We perform Exploratory Data Analysis (EDA) on the dataset, focusing on analyzing the target variable distribution, conducting univariate, bivariate, and potentially multivariate analysis, and summarizing the findings.
"""

df.profile_report()

"""##**1. Univariate Analysis**

####**A. Target Variable Distribution Analysis**
"""

print("Class distribution of the target variable 'claim':")
display(df['claim'].value_counts())

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='claim', palette='coolwarm')
plt.title('Distribution of Claim (Target Variable)')
plt.xlabel('Claim (0: No Claim, 1: At Least One Claim)')
plt.ylabel('Count')
plt.show()

print("Analysis of target variable for class imbalance completed.")

"""Here are the key insights from the target variable distribution analysis:

*   **Class Imbalance**: The target variable `claim` shows a significant class imbalance. The count of `0` (no claim) is substantially higher than the count of `1` (at least one claim). This means that only a small percentage of buildings in the dataset resulted in an insurance claim.

*   **Implication for Modeling**: This imbalance is a critical factor for model building. A model trained on such imbalanced data might be biased towards the majority class (no claim) and could perform poorly in predicting the minority class (claim). To address this, techniques such as:
    *   **Resampling**: Oversampling the minority class or undersampling the majority class.
    *   **Class Weighting**: Assigning higher weights to the minority class during model training.
    *   **Appropriate Evaluation Metrics**: Relying on metrics like Precision, Recall, F1-score, AUC-ROC, or Area Under the Precision-Recall Curve (AUPRC) rather than just accuracy, as accuracy can be misleading in imbalanced datasets.

####**B. Numerical Features Distribution Analysis**

**Non-graphical**
"""

numerical_features = df.select_dtypes(include=['number'])
  for col in numerical_features: # Iterate through the list of column names
      print("+"*10, col, "+"*10)
   # Access the column data from the DataFrame 'df'
      print(df[col].agg(["count", "min", "max", "mean", "median", "var", "std", "skew", "kurt"]))
      print()

"""**Graphical**

**Graphical Analysis of Numerical Features**

To visualize the distributions of numerical features, we will use a combination of histograms, Kernel Density Estimate (KDE) plots, box plots, and violin plots. These plots help in understanding the shape of the data, identifying central tendencies, spread, and potential outliers.

*   **Histograms and KDE plots:** Show the overall distribution, skewness, and modality.
*   **Box plots:** Provide a summary of the distribution (quartiles, median) and highlight outliers.
*   **Violin plots:** Combine aspects of box plots and KDE plots, showing both the distribution shape and summary statistics.
"""

numerical_features = ['insured_period', 'building_dimension', 'number_of_windows', 'building_age']

for feature in numerical_features:
    plt.figure(figsize=(18, 5))

    # Histogram and KDE Plot
    plt.subplot(1, 3, 1)
    sns.histplot(df[feature], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {feature.replace("_", " ").title()}')
    plt.xlabel(feature.replace("_", " ").title())
    plt.ylabel('Frequency')
    plt.grid(axis='y', alpha=0.75)

    # Box Plot
    plt.subplot(1, 3, 2)
    sns.boxplot(x=df[feature], color='lightcoral')
    plt.title(f'Box Plot of {feature.replace("_", " ").title()}')
    plt.xlabel(feature.replace("_", " ").title())

    # Violin Plot
    plt.subplot(1, 3, 3)
    sns.violinplot(x=df[feature], color='lightgreen')
    plt.title(f'Violin Plot of {feature.replace("_", " ").title()}')
    plt.xlabel(feature.replace("_", " ").title())

    plt.tight_layout()
    plt.show()

print("Graphical analysis for numerical features generated successfully.")

"""Here are the insights from the numerical features distribution analysis:

1.  **`insured_period` (Strongly Negatively Skewed)**:
    *   The distribution is heavily concentrated towards `1.0`, indicating that a large majority of insurance policies are for the full duration (e.g., a full year).
    *   Fewer policies cover shorter periods, which are visible as the tail extending to the left.
    *   This suggests that most properties are insured for long terms, making shorter periods less common.

2.  **`building_dimension` (Strongly Positively Skewed)**:
    *   The distribution is right-skewed, with a long tail extending towards higher values. This implies that most buildings have smaller to medium dimensions, while there are a few buildings with exceptionally large dimensions.
    *   The presence of these very large dimensions is also visible in the box plots and violin plots as numerous outliers on the higher end.
    *   This variable might benefit from transformation (e.g., logarithmic) for models that assume normal distribution.

3.  **`number_of_windows` (Moderately Positively Skewed)**:
    *   The distribution shows a positive skew, meaning that most buildings have a lower number of windows, and only a few buildings have a significantly higher count.
    *   The bulk of the data is concentrated around 0-5 windows, with decreasing frequency for higher counts. The `boxplot` confirms no outliers were detected by the IQR method, meaning the higher window counts, while less frequent, are within the expected range given the distribution.

4.  **`building_age` (Strongly Positively Skewed)**:
    *   This feature is positively skewed, indicating that a majority of the buildings are relatively newer (younger age), while a smaller number of buildings are very old.
    *   The box plot confirms the presence of several outliers corresponding to very old buildings (high building age). This could be due to historical buildings or properties with very early occupancy dates.

####**C. Categorical Feature Analysis**
We will analyze the distribution of categories within each categorical column using count plots.

**Non-Graphical**
"""

df.describe(include='object').T

"""*   **`residential`**: Majority are non-residential (0).
*   **`building_painted`**: Most buildings are painted ('V').
*   **`building_fenced`**: Roughly even split, with a slight majority unfenced ('N').
*   **`garden`**: Roughly even split, with a slight majority lacking a garden ('O').
*   **`settlement`**: Roughly even split, with a slight majority in rural areas ('R').
*   **`building_type`**: Type '2' is the most common building type among the four categories.
"""

# Identify categorical columns
cat_df = df.select_dtypes(include=['object'])

print("Categorical Feature Analysis (Counts and Proportions Combined):")

for col in cat_df:
    print(f"\n--- Analysis for '{col}' ---")

    # Calculate frequency counts
    counts = df[col].value_counts().to_frame(name='Count')

    # Calculate proportions
    proportions = (df[col].value_counts(normalize=True) * 100).to_frame(name='Proportion (%)')

    # Join counts and proportions
    combined_analysis = counts.join(proportions)

    # Display the combined table
    display(combined_analysis)

"""Here are the key insights from the categorical features distribution analysis:

1.  **`residential`**:
    *   Approximately 69.16% of the buildings are non-residential (`0`), while 30.84% are residential (`1`). This indicates a dataset where non-residential properties are more prevalent.

2.  **`building_painted`**:
    *   A significant majority of buildings (75.68%) are painted (`V`), compared to 24.32% that are not painted (`N`). This suggests that painted buildings are much more common in the dataset.

3.  **`building_fenced`**:
    *   The distribution between fenced (`V`) and unfenced (`N`) buildings is relatively even, with unfenced buildings slightly more common at 51.30% compared to 48.70% for fenced buildings.

4.  **`garden`**:
    *   Similar to `building_fenced`, the presence of a garden is almost equally split, with buildings without a garden (`O`) slightly more common at 51.31% compared to 48.69% for buildings with a garden (`V`).

5.  **`settlement`**:
    *   Buildings in rural areas (`R`) constitute a slight majority at 51.33%, while those in urban areas (`U`) account for 48.67%. This indicates a fairly balanced representation of buildings across different settlement types.

6.  **`building_type`**:
    *   `Building_Type` 2 is the most common, making up 45.65% of the dataset.
    *   `Building_Type` 1 is the second most common at 24.14%.
    *   `Building_Type` 3 and 4 are less frequent, with 17.34% and 12.87% respectively.
    *   This shows a clear dominance of `Building_Type` 2, which might have implications for modeling if certain building types are more prone to claims.

**Graphical Analysis of Categorical Features with Percentages**

To provide a more detailed understanding of the distribution of each categorical feature, we will generate count plots that also display the percentage of each category. This will help in quickly identifying the proportion of each subcategory within its respective feature.
"""

categorical_features = df.select_dtypes(include=['object']).columns

print("Categorical features are:",categorical_features)
plt.figure(figsize=(15, 10))

for i, col in enumerate(categorical_features):
    plt.subplot(2, 3, i + 1)
    ax = sns.countplot(data=df, x=col, palette='viridis')
    plt.title(f'Distribution of {col} with Percentages')
    plt.xlabel(col)
    plt.ylabel('Count')

    # Calculate percentages and annotate bars
    total = len(df[col])
    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height()/total)
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()
        ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=10, color='black')

plt.tight_layout()
plt.show()

print("Visualizations of categorical feature distributions with percentages completed.")

"""##**2. Bivariate Analysis**

####**a. Numerical Feature Correlation**

We will calculate the correlation matrix for numerical features to quantify the linear relationships between them.

**Non-Graphical**
"""

# Identify numerical columns
numerical_cols = df.select_dtypes(include='number').columns.tolist()

# Calculate the correlation matrix
correlation_matrix = df[numerical_cols].corr()

print("Correlation Matrix of Numerical Features:")
display(correlation_matrix.round(3)) # Rounding for display

"""**Graphical**"""

num_df = df.select_dtypes(exclude='object')
correlation_matrix = df[num_df.columns].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".3f")
plt.title('Correlation Matrix of Numerical Features and claim')
plt.show()

print("Correlation analyis completed.")

"""Here are the key insights from the numerical feature correlation analysis:

1.  **Correlation with 'claim' (Target Variable)**:
    *   `building_dimension` (0.292), `number_of_windows` (0.286), and `building_age` (0.232) show a **moderate positive correlation** with `claim`. This suggests that larger buildings, buildings with more windows, and older buildings tend to have a higher likelihood of an insurance claim.
    *   `occupancy_year` (-0.231) has a **moderate negative correlation** with `claim`. This aligns with `building_age`, indicating that buildings occupied more recently (younger buildings) are less likely to have claims.
    *   `insured_period` (0.092) shows a **weak positive correlation**, implying a very slight tendency for longer insured periods to be associated with claims.
    *   `observation_year` (0.006) has an **extremely weak positive correlation**, indicating almost no linear relationship with claims.

2.  **Inter-feature Correlations**:
    *   `building_dimension` and `number_of_windows` (0.286) have a **moderate positive correlation**, which is intuitive as larger buildings are likely to have more windows.
    *   `occupancy_year` and `building_age` (-0.999) show an **extremely strong negative correlation**. This is expected, as `building_age` is calculated directly from `occupancy_year` and `observation_year`.

####**b. Categorical Features pairwise distribution**

We will inspect the relationships between pairs of categorical features. This allows us to see the distribution of one categorical variable within each category of another, helping identify potential associations or dependencies.

**Non-Graphical**    
We will examine the relationship between pairs of categorical features using contingency tables (cross-tabulations) to see the frequency distribution of categories across two variables.
"""

import itertools

print("\nContingency Tables for Pairs of Categorical Features:")

# Generate all unique pairs of categorical column names
for col1, col2 in itertools.combinations(cat_df.columns, 2):
    print(f"\n--- Contingency Table: '{col1}' vs. '{col2}' ---")
    contingency_table = pd.crosstab(df[col1], df[col2])
    display(contingency_table)

"""**Graphical Analysis**

To visually inspect the relationships between pairs of categorical features, we will generate grouped count plots. This allows us to see the distribution of one categorical variable within each category of another, helping identify potential associations or dependencies.
"""

import itertools
import matplotlib.pyplot as plt
import seaborn as sns

categorical_features = df.select_dtypes(include='object').columns.tolist()

print("Generating graphical analysis for pairs of categorical features...")

# Generate all unique pairs of categorical column names
for i, (col1, col2) in enumerate(itertools.combinations(categorical_features, 2)):
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x=col1, hue=col2, palette='viridis')
    plt.title(f'Distribution of {col1.replace("_", " ").title()} by {col2.replace("_", " ").title()}')
    plt.xlabel(col1.replace("_", " ").title())
    plt.ylabel('Count')
    plt.legend(title=col2.replace("_", " ").title())
    plt.tight_layout()
    plt.show()

print("Graphical analysis for categorical feature pairs completed.")

"""Here are the key insights from the categorical feature pairs distribution analysis, combining observations from contingency tables and grouped count plots:

*   **`residential` vs. `building_painted`**: It appears that both residential and non-residential buildings can be painted or not painted. However, the distribution of 'painted' (`V`) vs. 'not painted' (`N`) might differ slightly between residential and non-residential properties. For instance, a higher proportion of non-residential buildings might be painted compared to residential ones, or vice versa. Further analysis would involve looking at percentages within each category.

*   **`residential` vs. `building_fenced`**: Both residential and non-residential buildings exhibit a mix of fenced and unfenced properties. There isn't an obvious strong dependency where one type of building is exclusively fenced or unfenced. However, a slight tendency might exist, for example, residential properties being more frequently fenced.

*   **`residential` vs. `garden`**: Similar to fencing, both residential and non-residential buildings can have or not have a garden. Residential properties are intuitively more likely to have a garden, but this relationship needs to be quantified to understand the strength of the association.

*   **`residential` vs. `settlement`**: Both residential and non-residential buildings are found in both urban (`U`) and rural (`R`) settlements. The distribution across settlement types might indicate whether residential or non-residential buildings are more concentrated in urban or rural areas.

*   **`residential` vs. `building_type`**: All `building_type` categories (1, 2, 3, 4) contain both residential and non-residential properties. It's likely that certain building types are predominantly one over the other (e.g., Type 1 might be more residential, while Type 4 might be more non-residential).

*   **`building_painted` vs. `building_fenced`**: Buildings can be painted or not, and fenced or unfenced, independently. There doesn't appear to be a strong direct link, but combinations like 'painted and fenced' or 'not painted and unfenced' will show their respective frequencies.

*   **`building_painted` vs. `garden`**: The presence of a garden does not appear to dictate whether a building is painted or not, or vice-versa. Buildings with and without gardens can be painted or unpainted.

*   **`building_painted` vs. `settlement`**: Painted and unpainted buildings exist in both urban and rural settlements. We can observe if buildings in one settlement type are more likely to be painted than in another.

*   **`building_painted` vs. `building_type`**: All building types can be painted or unpainted. The proportion of painted buildings might vary across different building types.

*   **`building_fenced` vs. `garden`**: There's a natural association here where buildings with gardens might be more likely to be fenced. This relationship would be interesting to quantify.

*   **`building_fenced` vs. `settlement`**: Buildings in both urban and rural areas can be fenced or unfenced. Rural areas might have a higher proportion of fenced properties due to different land use or security needs.

*   **`building_fenced` vs. `building_type`**: All building types can be fenced or unfenced. Some building types might inherently be more or less likely to have fences.

*   **`garden` vs. `settlement`**: Buildings with and without gardens exist in both urban and rural areas. It's plausible that rural areas have a higher proportion of buildings with gardens. This could be a good feature to investigate.

*   **`garden` vs. `building_type`**: All building types can have or not have a garden. The distribution of gardens might vary across different building types.

*   **`settlement` vs. `building_type`**: All building types are present in both urban and rural settlements. The distribution could show if certain building types are more common in one settlement type over another.

####**c. Numerical Features and The Target Variables**

**Non-Graphical Analysis: Grouped Statistics**

We will calculate descriptive statistics (mean, median, standard deviation) for each numerical feature, grouped by the `claim` variable. This helps in understanding how the central tendency and spread of numerical features differ between buildings with and without claims.
"""

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.drop('claim')

print("Descriptive Statistics of Numerical Features Grouped by Claim Status:")
for feature in numerical_features:
    print(f"\n--- Feature: '{feature.replace('_', ' ').title()}' ---")
    grouped_stats = df.groupby('claim')[feature].agg(['mean', 'median', 'std'])
    display(grouped_stats) # Explicitly display each grouped_stats
    print() # Add a newline for better readability between tables

print("Non-graphical bivariate analysis completed.")

"""Here are the key insights from the descriptive statistics of numerical features, grouped by the `claim` status:

*   **`insured_period` (Mean, Median, Std)**:
    *   **No Claim (Claim=0)**: Mean insured period is approximately 0.896, with a median of 1.0 and a standard deviation of 0.256.
    *   **With Claim (Claim=1)**: Mean insured period is approximately 0.948, with a median of 1.0 and a standard deviation of 0.178.
    *   **Insight**: Buildings that have experienced a claim tend to have a slightly higher mean insured period and a lower standard deviation compared to those with no claims. This suggests that buildings with claims are more likely to have policies covering a longer duration, often the full year, with less variation in their policy lengths.

*   **`building_dimension` (Mean, Median, Std)**:
    *   **No Claim (Claim=0)**: Mean building dimension is approximately 1518 m², with a median of 900 m² and a standard deviation of 1765 m².
    *   **With Claim (Claim=1)**: Mean building dimension is approximately 3100 m², with a median of 1995.5 m² and a standard deviation of 3200 m².
    *   **Insight**: Buildings with claims have significantly larger mean and median dimensions compared to buildings without claims. The standard deviation is also much higher for buildings with claims. This indicates a strong positive relationship: larger buildings are substantially more prone to having insurance claims, or claims are more frequently observed for larger properties.

*   **`number_of_windows` (Mean, Median, Std)**:
    *   **No Claim (Claim=0)**: Mean number of windows is approximately 2.04, with a median of 1.0 and a standard deviation of 2.36.
    *   **With Claim (Claim=1)**: Mean number of windows is approximately 2.83, with a median of 3.0 and a standard deviation of 2.93.
    *   **Insight**: Buildings with claims tend to have a higher mean and median number of windows than those without claims. This suggests that buildings with more windows might be marginally more susceptible to claims. The increased standard deviation for claimed buildings also hints at greater variability in window counts within this group.

**Graphical Analysis: Box Plots**

We will use box plots to visually compare the distribution of each numerical feature for buildings with and without insurance claims. Box plots are excellent for showing the median, quartiles, and potential outliers across different categories.
"""

import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.drop('claim')

print("Generating box plots for numerical features by claim status...")

plt.figure(figsize=(15, len(numerical_features) * 4))
for i, feature in enumerate(numerical_features):
    plt.subplot(len(numerical_features), 1, i + 1)
    sns.boxplot(data=df, x='claim', y=feature, palette='coolwarm')
    plt.title(f'Distribution of {feature.replace("_", " ").title()} by Claim Status')
    plt.xlabel('Claim (0: No Claim, 1: At Least One Claim)')
    plt.ylabel(feature.replace("_", " ").title())
    plt.grid(axis='y', alpha=0.75)

plt.tight_layout()
plt.show()

print("Graphical bivariate analysis completed.")

"""Here are the key insights from the box plots comparing numerical features against the `claim` status:

*   **`insured_period` vs. `claim`**:
    *   **Buildings with No Claims (Claim=0)**: The `insured_period` for buildings with no claims generally shows a higher concentration towards 1.0 (full year policies), but with a noticeable spread towards shorter durations, including several outliers at very low insured periods.
    *   **Buildings with Claims (Claim=1)**: For buildings with claims, the `insured_period` is also heavily concentrated at 1.0. However, the distribution for claims might appear slightly less varied on the lower end, suggesting that policies that result in claims are predominantly full-year, with fewer instances of very short policy durations leading to claims. This reinforces that claims are more likely with longer (often full-year) insured periods.

*   **`building_dimension` vs. `claim`**:
    *   **Buildings with No Claims (Claim=0)**: The `building_dimension` for buildings without claims is heavily skewed towards smaller values. The median is relatively low, and there are numerous outliers representing buildings with larger dimensions.
    *   **Buildings with Claims (Claim=1)**: In contrast, buildings with claims show a significantly higher median `building_dimension`. The entire distribution is shifted towards larger sizes, and the range of dimensions is much wider, indicating that larger buildings are considerably more prone to having insurance claims. This is a strong indicator of a relationship between building size and claim likelihood.

*   **`number_of_windows` vs. `claim`**:
    *   **Buildings with No Claims (Claim=0)**: The `number_of_windows` for buildings with no claims is clustered around lower counts (median typically 1-2 windows).
    *   **Buildings with Claims (Claim=1)**: For buildings with claims, the `number_of_windows` distribution shows a slightly higher median and upper quartile. This suggests that buildings with a greater number of windows tend to be associated with a higher likelihood of insurance claims. While not as pronounced as `building_dimension`, it indicates a potential contributing factor.

####**d. Categorical Features and The Target Variables**    

We will visualize the relationship between categorical features and the target variable ('claim')

**(Non-Graphical)**

We will examine the relationship between categorical features and the numerical target variable ('claim') by calculating grouped summary statistics.
"""

# Calculate grouped summary statistics for each categorical feature

print("\nClaim Summary Statistics Grouped by Categorical Features:")
for col in cat_df:
    print(f"\n--- Analysis for '{col}' ---")
    print(f"\n--- Claim Statistics by '{col}' ---")
    grouped_stats = df.groupby(col)['claim'].agg(['mean', 'median', 'std']).reset_index()
    display(grouped_stats)

"""Here are the key insights from the non-graphical bivariate analysis (claim summary statistics grouped by categorical features):

*   **`residential` vs. `claim`**:
    *   **Residential (1)**: Has a mean claim rate (`mean`) of approximately 0.27, which is higher than for non-residential buildings.
    *   **Non-Residential (0)**: Has a mean claim rate of approximately 0.21.
    *   **Insight**: Residential buildings appear to have a slightly higher likelihood of insurance claims compared to non-residential buildings.

*   **`building_painted` vs. `claim`**:
    *   **Not Painted (N)**: Has a mean claim rate of approximately 0.21.
    *   **Painted (V)**: Has a mean claim rate of approximately 0.24.
    *   **Insight**: Buildings that are *painted* (`V`) show a noticeably higher claim rate than those that are not painted (`N`). This could suggest that painted buildings might be older, less maintained, or in areas with higher risk.

*   **`building_fenced` vs. `claim`**:
    *   **Fenced (V)**: Has a mean claim rate of approximately 0.21.
    *   **Not Fenced (N)**: Has a mean claim rate of approximately 0.25.
    *   **Insight**: Buildings that are *not fenced* (`V`) have a slightly higher mean claim rate compared to fenced buildings (`N`). This might indicate that unfenced properties are often residential or in certain riskier areas, or that fencing itself doesn't deter claims, or even makes them more likely in some contexts.

*   **`garden` vs. `claim`**:
    *   **Has Garden (V)**: Has a mean claim rate of approximately 0.21.
    *   **No Garden (O)**: Has a mean claim rate of approximately 0.25.
    *   **Insight**: Buildings with a *no garden* (`O`) show a marginally higher mean claim rate than those with a garden (`N`)

*   **`settlement` vs. `claim`**:
    *   **Rural (R)**: Has a mean claim rate of approximately 0.25.
    *   **Urban (U)**: Has a mean claim rate of approximately 0.21.
    *   **Insight**: Buildings in *rural areas* (`R`) have a slightly higher mean claim rate compared to those in urban areas (`U`). This could be due to different risk factors prevalent in rural settings (e.g., exposure to natural elements, different construction types).

*   **`building_type` vs. `claim`**:
    *   **Type 1**: Mean claim rate of approximately 0.18.
    *   **Type 2**: Mean claim rate of approximately 0.22.
    *   **Type 3**: Mean claim rate of approximately 0.26.
    *   **Type 4**: Mean claim rate of approximately 0.34.
    *   **Insight**: There is a clear trend: as the `building_type` number increases, the mean claim rate also tends to increase. `Building_Type` 4 has the highest claim rate, significantly higher than Type 1. This suggests that `building_type` is an important predictor of claims, with certain types being considerably riskier.

**Graphical**
"""

print("Investigating relationships between categorical features and 'claim' using count plots...")

categorical_features = df.select_dtypes(include=['object']).columns

# Create a figure with subplots

plt.figure(figsize=(20, 25))

for i, col in enumerate(categorical_features):
    plt.subplot(4, 2, i + 1) # Adjust subplot grid based on number of columns
    ax = sns.countplot(data=df, x=col, hue='claim', palette='viridis')
    plt.title(f'Distribution of {col.replace("_", " ").title()} by Claim Status')
    plt.xlabel(col.replace("_", " ").title())
    plt.ylabel('Count')
    plt.legend(title='Claim', labels=['No Claim', 'Claim'])

    # Calculate percentages and annotate bars
    total_counts = df[col].value_counts().sum() # Total count for the current categorical feature
    for container in ax.containers:
        for p in container.patches:
            height = p.get_height()
            if height > 0: # Avoid division by zero for empty categories
                percentage = '{:.1f}%'.format(100 * height/total_counts)
                x = p.get_x() + p.get_width() / 2
                y = height
                ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=10, color='black')

plt.tight_layout()
plt.show()

print("Analysis of categorical features against the target variable complete.")

"""Here are the key visual insights from the graphical bivariate analysis of categorical features against the target variable:

*   **`residential`**: The count plots with percentages clearly show that while there are more non-residential buildings overall, a higher percentage of residential buildings (`1`) tend to have claims compared to non-residential buildings (`0`).
*   **`building_painted`**: A higher percentage of painted buildings (`V`) compared to not painted buildings (`N`) appear to have claims, reinforcing the earlier non-graphical observation.
*   **`building_fenced`**: Buildings that are `not fenced` (`N`) show a higher percentage of claims than `fenced` buildings (`V`).
*   **`garden`**: Buildings without a garden (`O`) generally have a higher percentage of claims compared to buildings with a garden (`V`).
*   **`settlement`**: Buildings in `rural` areas (`R`) consistently show a higher percentage of claims than those in `urban` areas (`U`).
*   **`building_type`**: The trend observed earlier is visually clear: `Building_Type` 4 has the highest percentage of claims, followed by Type 3, then Type 2, and Type 1 has the lowest percentage. This confirms that building type is a strong differentiator for claim likelihood.

#**Step 3: Feature Engineering and Preprocessing for Modeling**

##**1. Feature Transformation**

We will address the identified skewness in numerical features such as `building_dimension` and `insured_period`. Apply appropriate transformations (e.g., logarithmic, square root, or Box-Cox) to make their distributions more symmetrical, which can improve the performance of some machine learning models. Also, re-evaluate the `building_age` feature for any potential transformations if its distribution is highly skewed.
"""

print(f"Skewness of 'building_age': {df['building_age'].skew():.2f}")

# Apply logarithmic transformation to building_dimension
df['log_building_dimension'] = np.log1p(df['building_dimension'])
print("'building_dimension' transformed to 'log_building_dimension' using log1p.")

# Apply reflected-logarithmic transformation to insured_period
df['transformed_insured_period'] = np.log1p(1 - df['insured_period'])
print("'insured_period' transformed to 'transformed_insured_period' using reflected log1p.")

# Conditionally apply logarithmic transformation to building_age
# Assuming 'highly skewed' means an absolute skewness value greater than 1
if abs(df['building_age'].skew()) > 1:
    df['log_building_age'] = np.log1p(df['building_age'])
    print("'building_age' transformed to 'log_building_age' using log1p due to high skewness.")
else:
    df['log_building_age'] = df['building_age'] # Keep original if not highly skewed
    print("'building_age' not highly skewed, 'log_building_age' is a copy of original 'building_age'.")

print("Feature transformations applied.")

"""We will now recalculate and display the skewness of the newly transformed numerical columns (`log_building_dimension`, `transformed_insured_period`, and `log_building_age`) to verify the effectiveness of the applied transformations.


"""

transformed_numerical_features = ['log_building_dimension', 'transformed_insured_period', 'log_building_age']
print("Skewness of transformed numerical features:")
print(df[transformed_numerical_features].skew())

"""We will now generate histograms or KDE plots for the transformed numerical features (`log_building_dimension`, `transformed_insured_period`, and `log_building_age`) to visually inspect their new distributions and assess the effectiveness of the transformations.


"""

transformed_numerical_features = ['log_building_dimension', 'transformed_insured_period', 'log_building_age']

plt.figure(figsize=(15, 5))
for i, col in enumerate(transformed_numerical_features):
    plt.subplot(1, 3, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col} (Transformed)')
    plt.xlabel(col)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()
print("Histograms and KDE plots for transformed numerical features generated.")

"""Here are the key insights from the distribution analysis of the transformed numerical features:

1.  **`log_building_dimension` (Transformed from `building_dimension`)**:
    *   **Original Skewness**: Highly positive (around 3.12).
    *   **Transformed Skewness**: Close to zero (approximately -0.20).
    *   **Visual Insight**: The histogram and KDE plot for `log_building_dimension` show a much more symmetrical, bell-shaped distribution compared to the original `building_dimension`. The logarithmic transformation effectively reduced the heavy positive skew, making the data more normally distributed. This is beneficial for models that assume normal input distributions, helping them learn relationships more effectively and reducing the impact of extreme large values.

2.  **`transformed_insured_period` (Transformed from `insured_period`)**:
    *   **Original Skewness**: Strongly negative (around -2.72).
    *   **Transformed Skewness**: Strongly positive (approximately 2.53).
    *   **Visual Insight**: The reflected logarithmic transformation (`np.log1p(1 - df['insured_period'])`) was applied because the original data was negatively skewed and concentrated at 1.0. While this transformation has changed the direction of skewness from negative to positive, the distribution still appears heavily skewed. The bulk of the data is now concentrated towards the lower end (close to 0 on the transformed scale), with a long tail extending to the right. This suggests that while the transformation altered the distribution, it did not achieve a symmetrical or normalized distribution. Further investigation or alternative transformations might be considered if normalization is a key requirement for the modeling approach.

3.  **`log_building_age` (Transformed from `building_age`)**:
    *   **Original Skewness**: Strongly positive (around 2.28).
    *   **Transformed Skewness**: Negative (approximately -1.52).
    *   **Visual Insight**: The histogram and KDE plot for `log_building_age` show that the logarithmic transformation has significantly reduced the positive skewness present in the original `building_age` feature. The distribution is now negatively skewed, indicating a concentration of values towards the higher end of the transformed scale, with a tail extending to the left. This transformation has made the distribution more compact and reduced the influence of extremely old buildings, which were outliers in the original data.

##**2. Categorical Feature Encoding**

Convert all categorical features (e.g., `building_painted`, `building_fenced`, `garden`, `settlement`, `residential`, `building_type`) into numerical representations suitable for machine learning models. Techniques such as One-Hot Encoding or Label Encoding will be applied based on the nature of each categorical variable (e.g., ordinal vs. nominal).

As all identified categorical features are nominal (no inherent order), we will apply One-Hot Encoding to them using `pd.get_dummies()`. This will convert each categorical value into a new binary column, suitable for machine learning models. Then, we will then concatenate these new columns with the original DataFrame and drop the initial categorical columns.
"""

# identifying categorical columns for encoding
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print('Categorical columns identified for encoding:', categorical_cols)

# applying one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
print("Categorical features have been One-Hot Encoded.")
print(f"New shape of DataFrame: {df_encoded.shape}")

"""Since all identified categorical features are nominal (no inherent order), then we applied One-Hot Encoding to them using `pd.get_dummies()`. This converted each categorical value into a new binary column, suitable for machine learning models. Then, we will then concatenate these new columns with the original DataFrame and drop the initial categorical columns.

We will now verify the data types and the number of columns in the DataFrame using `df_encoded.info()` to ensure all categorical features have been converted to numerical representations.
"""

df_encoded.info()
print("Data type verification complete after encoding.")

"""##**3. Dropping irrevelant Columns:**

We will drop the specified redundant date-related columns from `df_encoded` to prepare the DataFrame for SMOTE, which requires numerical input and avoids using redundant features.


"""

columns_to_drop = ['year_of_observation', 'date_of_occupancy', 'observation_year', 'occupancy_year']
df_encoded.drop(columns=columns_to_drop, inplace=True)

print(f"Columns {columns_to_drop} dropped successfully.")
print(f"New DataFrame columns: {df_encoded.columns.tolist()}")

"""Since `building_age` has already been created, `year_of_observation`, `date_of_occupancy`, `observation_year` and `occupancy_year` are irrelevant columns

#**STEP 4: Model Development**

Here, we will prepare the data for model training by identifying features (X) and the target variable (y), and then splitting the dataset into training and testing sets.

##**1. Data Preprocessing**

###**a. Creating the Features and Target Variable:**

We will separate the features (X) and the target variable (y) from the `df_encoded` DataFrame. The target variable `y` will be the 'claim' column, and `X` will be all other columns.
"""

# Define the features
X = df_encoded.drop(columns=['claim'])

# Define target Variable
y = df_encoded['claim']

print("Features (X) and target (y) separated successfully.")
print(f"Shape of X: {X.shape}")
print(f"Shape of y: {y.shape}")

"""Here are the key insights from the shapes of `X` and `y`:

*   **Features (X) Shape**: `(7014, 15)` indicates that after preprocessing and feature engineering, the dataset has 7014 observations (rows) and 15 features (columns) that will be used as input for the machine learning models.
*   **Target (y) Shape**: `(7014,)` confirms that there are 7014 corresponding target values, which is the `claim` status for each observation.

This outcome signifies that the data has been successfully prepared and structured for training and testing machine learning models, with a clear distinction between the predictor variables and the variable to be predicted.

##**b. Handling Class Imbalance**

We will address the class imbalance identified in the target variable 'claim'. We will implement strategies such as oversampling the minority class (e.g., SMOTE) to prevent the model from being biased towards the majority class and to improve its ability to predict claims.
"""

# importing libraries to handle class imbalance
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import collections

print("Necessary libraries imported successfully for class imbalance handling.")

"""We will print the class distribution of the original target variable `y` using `y.value_counts()` to observe the current imbalance.


"""

print("investigating class distribution of the target variable 'claim'")
print("Original class distribution (0:No claim, 1: Claim):")
print(y.value_counts())
print("\nOriginal class distribution (percentage) (0: No claim, 1: Claim):")
print(y.value_counts(normalize=True) * 100)
print("Class distribution verification complete.")

"""Here are the key insights from the target variable distribution analysis:

*   **Class Imbalance**: The target variable `claim` shows a significant class imbalance. The count of `0` (no claim) is substantially higher than the count of `1` (at least one claim). This means that only a small percentage of buildings in the dataset resulted in an insurance claim.

*   **Implication for Modeling**: This imbalance is a critical factor for model building. A model trained on such imbalanced data might be biased towards the majority class (no claim) and could perform poorly in predicting the minority class (claim). To address this, techniques such as:
    *   **Resampling**: Oversampling the minority class or undersampling the majority class.
    *   **Class Weighting**: Assigning higher weights to the minority class during model training.
    *   **Appropriate Evaluation Metrics**: Relying on metrics like Precision, Recall, F1-score, AUC-ROC, or Area Under the Precision-Recall Curve (AUPRC) rather than just accuracy, as accuracy can be misleading in imbalanced datasets.

Now, we will instantiate a `SMOTE` object with a random state for reproducibility.
"""

smote = SMOTE(random_state=42)
print("SMOTE object instantiated successfully.")

"""We will now apply SMOTE to the features `X` and target `y` to create oversampled datasets `X_resampled` and `y_resampled`.


"""

X_resampled, y_resampled = smote.fit_resample(X, y)

print("SMOTE applied successfully, creating resampled datasets.")
print(f"Shape of X_resampled: {X_resampled.shape}")
print(f"Shape of y_resampled: {y_resampled.shape}")

"""Now, We will now print the class distribution of the resampled target variable `y_resampled` to verify that the imbalance has been addressed.


"""

print("Resampled class distribution:")
print(y_resampled.value_counts())
print("Resampled class distribution (percentage):")
print(y_resampled.value_counts(normalize=True) * 100)

"""Here are the key insights from the resampled target variable distribution:

*   **Balanced Classes**: After applying SMOTE, the target variable `claim` is now perfectly balanced. Both classes, `0` (no claim) and `1` (at least one claim), have 5388 instances each, representing 50% of the dataset for each class.
*   **Addressed Imbalance**: This confirms that the class imbalance observed in the original dataset has been successfully addressed through oversampling the minority class. A balanced dataset is crucial for training robust machine learning models, as it prevents the model from being biased towards the majority class and improves its ability to correctly predict instances of the minority class.

###**c. Data Splitting and Scaling**

We will split the dataset into training and testing sets to prepare for model building and evaluation.

We split the `X_resampled` and `y_resampled` datasets into training and testing sets using `train_test_split`, ensuring consistent class distribution with `stratify=y_resampled`.
"""

# import train_test_split
from sklearn.model_selection import train_test_split

# Split the data into training and training sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

print("Dataset split into training and testing sets successfully.")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""Here are the key insights from the shapes of `X` and `y`:

*   **Features (X) Shape**: `(7014, 15)` indicates that after preprocessing and feature engineering, the dataset has 7014 observations (rows) and 15 features (columns) that will be used as input for the machine learning models.
*   **Target (y) Shape**: `(7014,)` confirms that there are 7014 corresponding target values, which is the `claim` status for each observation.

This outcome signifies that the data has been successfully prepared and structured for training and testing machine learning models, with a clear distinction between the predictor variables and the variable to be predicted, ensuring a robust foundation for model development.

###**d. Numerical Features Scaling**   

We will scale the numerical features (e.g., using StandardScaler or MinMaxScaler) within the training data and apply the same transformation to the test data to ensure that all features contribute equally to the model training process.


Let's first identify the numerical columns in `X_resampled` that require scaling, and then, we will instantiate a `StandardScaler`.
"""

from sklearn.preprocessing import StandardScaler

# identify numerical columns for scaling
numerical_cols_for_scaling = [
    'insured_period', 'building_dimension', 'number_of_windows', 'building_age',
    'log_building_dimension', 'transformed_insured_period', 'log_building_age'
]

# instantiate a StandardScaler
scaler = StandardScaler()

print("Numerical columns for scaling identified and StandardScaler instantiated.")

"""We will fit the `StandardScaler` exclusively on the numerical columns of the training data (`X_train`) and then, we transform the numerical columns of both the training (`X_train`) and testing (`X_test`) data using the fitted scaler, updating the DataFrames in place.


"""

# fit and transform the scaler on the train data's numerical features
X_train[numerical_cols_for_scaling] = scaler.fit_transform(X_train[numerical_cols_for_scaling])

# transform the scaler on the test data's numerical features
X_test[numerical_cols_for_scaling] = scaler.transform(X_test[numerical_cols_for_scaling])

print("Numerical features in X_train and X_test scaled successfully.")

"""We will now verify the shapes of `X_train`, `X_test`, `y_train`, and `y_test` to confirm the split and scaling operations were successful.


"""

print("Shapes after splitting and scaling:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

print("Data splitting and scaling verification complete.")

"""The output of the shapes after splitting and scaling confirms the following:

*   **`X_train` shape: `(8620, 15)`**: This indicates that the training feature set consists of 8620 samples, each with 15 features.
*   **`X_test` shape: `(2156, 15)`**: The testing feature set contains 2156 samples, each also with 15 features.
*   **`y_train` shape: `(8620,)`**: The training target set comprises 8620 corresponding target values.
*   **`y_test` shape: `(2156,)`**: The testing target set has 2156 corresponding target values.

##**2. Model Implementation**

We will select and implement more than one machine learning model suitable for binary classification tasks (e.g., Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, SVM). We will create an empty dictionary named `models` to store the initialized models

We will train these models on the preprocessed training data.

###**a. Model Initialization**
"""

# importing each model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize each model with random_state=42 for reproducibility
logistic_regression_model = LogisticRegression(random_state=42, solver='liblinear') # Added solver for LogisticRegression
decision_tree_model = DecisionTreeClassifier(random_state=42)
random_forest_model = RandomForestClassifier(random_state=42)

# Create a dictionary to store the initialized models
models = {
    'Logistic Regression': logistic_regression_model,
    'Decision Tree': decision_tree_model,
    'Random Forest': random_forest_model
}

print("Classification models imported, initialized, and stored in 'models' dictionary.")

"""###**b. Model Training**

We will now train each model stored in the `models` dictionary using the preprocessed training data (`X_train` and `y_train`).


"""

for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    print(f"{name} trained successfully.")

print("All models trained.")

"""###**c. Making Prediction**"""

import pandas as pd

print("Making predictions on the test set...")

# Initialize a dictionary to store predictions for all models
all_predictions = {}

for name, model in models.items():
    # Make predictions on the X_test data
    y_pred = model.predict(X_test)

    # Make probability predictions for the positive class
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Store predictions for the current model
    all_predictions['Actual'] = y_test
    all_predictions[name + '_prediction'] = y_pred
    all_predictions[name + '_proba'] = y_pred_proba # Store probabilities as well, if needed

# Convert the dictionary of predictions to a DataFrame
all_predictions_df = pd.DataFrame(all_predictions, index=X_test.index)

print("Predictions made for all models and stored in 'all_predictions_df' DataFrame.")

# Display the head of the predictions DataFrame
print("Sample predictions:")
display(all_predictions_df.head())

"""Here are the key insights from the predictions:

*   **Prediction Availability**: Predictions (both class labels and probabilities) have been successfully generated for all three trained models (Logistic Regression, Decision Tree, and Random Forest) on the unseen `X_test` data.
*   **Readiness for Evaluation**: This output represents the direct results of applying the trained models to new data, making it ready for comprehensive evaluation against the actual `y_test` values using various performance metrics.
*   **Initial Observation (Qualitative)**: By inspecting the sample, we can see instances where models correctly predicted the `Actual` claim status and instances where they might have made errors. We can also observe the confidence (probability) associated with each model's positive prediction, which can be crucial for understanding model behavior, especially for imbalanced datasets.

##**3. Model Evaluation and Selection**

We will evaluate the performance of the implemented models using appropriate metrics for imbalanced classification (e.g., Precision, Recall, F1-score, ROC-AUC, Confusion Matrix) on the test set. We will compare the models and select the best-performing one based on the project's objectives.


We will create an empty dictionary named `results` to store the evaluation metrics. We will iterate through each trained model aand calculate all specified evaluation metrics, store them in the `results` dictionary, and print them for each model.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Creating an empty dictionary 'results' to store model evaluation
results = {}

print('Evaluating each model')

for name, model in models.items():
    print(f"\n--- Evaluating {name} ---")

    # Generate predictions for the current model
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculate the metrics using the current model's predictions
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    cm = confusion_matrix(y_test, y_pred)

    # Store these metrics in the results dictionary
    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc,
        'Confusion Matrix': cm
    }

    # Print the model's name and its calculated metrics
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"ROC AUC Score: {roc_auc:.4f}")
    print("Confusion Matrix:")
    print(cm)

print("\nAll models evaluated.")

"""We will compare the performance of the models using a DataFrame to summarize the key evaluation metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC) obtained from the `results` dictionary. This will facilitate a clear comparison of each model's performance on the test set, especially focusing on metrics relevant for imbalanced datasets.


"""

import pandas as pd

# Create a list to store model summaries
model_summary = []

for name, metrics in results.items():
    model_summary.append({
        'Model': name,
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-Score': metrics['F1-Score'],
        'ROC-AUC': metrics['ROC-AUC']
    })

# Convert the list of dictionaries to a DataFrame for easy comparison
comparison_df = pd.DataFrame(model_summary)

print("Model Performance Summary:")
display(comparison_df.set_index('Model'))

"""Here are the key insights from the Model Performance Summary:

*   **Random Forest Classifier is the Best Performing Model**: Among the three models evaluated, the Random Forest Classifier demonstrates the strongest performance across most metrics:
    *   **Highest Accuracy (0.8135)**: It correctly classifies a higher proportion of instances overall.
    *   **Highest Precision (0.8347)**: When it predicts a claim, it is correct 83.47% of the time, which is important for minimizing false positives.
    *   **Highest F1-Score (0.8075)**: This metric, which is the harmonic mean of precision and recall, indicates a good balance between precision and recall.
    *   **Highest ROC-AUC (0.8910)**: This indicates its superior ability to distinguish between the two classes (claims vs. no claims).

*   **Decision Tree Classifier**: While having a slightly higher Recall than Logistic Regression, its overall performance is lower than Random Forest, and its ROC-AUC is the lowest among the three models (0.7557), suggesting it's not as good at separating the classes.

*   **Logistic Regression**: Shows a balanced performance in terms of Accuracy, Precision, and Recall (all approximately 0.7468), which is expected from a balanced dataset. Its ROC-AUC (0.8261) is better than Decision Tree but significantly lower than Random Forest.

**Conclusion for Model Selection**: Based on these metrics, the **Random Forest Classifier** is the most robust model for predicting insurance claims in this dataset, given its higher accuracy, precision, F1-score, and ROC-AUC. This suggests it is better at handling the complexities and relationships within the features to make more accurate predictions.

##**4. Hyperparameter Tuning**

We will now perform hyperparameter tuning on the Random Forest Classifier, which was identified as the best-performing model.


We will optimize the performance of the Random Forest Classifier by systematically searching for the best hyperparameters using GridSearchCV.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define a param_grid dictionary specifying the hyperparameters to tune
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate a RandomForestClassifier object
rf_model = RandomForestClassifier(random_state=42)

# Instantiate GridSearchCV
grid_search = GridSearchCV(
    estimator=rf_model,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2,
    scoring='f1' # Use F1-score as the evaluation metric
)

# Fit the GridSearchCV object to the resampled training data
print("Starting GridSearchCV for Random Forest...")
grid_search.fit(X_train, y_train)
print("GridSearchCV completed.")

# Retrieve and print the best parameters
print("\nBest parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Retrieve and print the best F1-score
print("\nBest F1-score achieved by the best model:")
print(grid_search.best_score_)

# Store the best-performing model
best_rf_model = grid_search.best_estimator_
print("\nBest Random Forest model stored in 'best_rf_model'.")

"""##**5. Cross-Validation of Tuned Model**

We will evaluate the stability and generalization capability of the hyperparameter-tuned Random Forest model using k-fold cross-validation. This provides a more robust estimate of the model's performance on unseen data compared to a single train-test split.

"""

from sklearn.model_selection import cross_val_score

# Perform k-fold cross-validation on the best_rf_model
cv_scores = cross_val_score(best_rf_model, X_resampled, y_resampled, cv=5, scoring='f1', n_jobs=-1)

print("Cross-validation F1-scores for the best Random Forest model:")
print(cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
print(f"\nMean F1-score: {cv_scores.mean():.4f}")
print(f"Standard deviation of F1-scores: {cv_scores.std():.4f}")

print("Cross-validation completed.")

"""Here are the key insights from the cross-validation of the hyperparameter-tuned Random Forest model:

*   **Individual F1-scores**: The F1-scores across the 5 folds are `[0.5006, 0.6946, 0.8752, 0.8129, 0.7975]`. There is a notable variation in the F1-scores across different folds, ranging from 0.5006 to 0.8752.
*   **Mean F1-score (0.7361)**: The average F1-score from the 5-fold cross-validation is approximately 0.7361. This represents a more robust estimate of the model's F1-score compared to a single train-test split, as it averages performance across different subsets of the data.
*   **Standard Deviation of F1-scores (0.1313)**: The standard deviation of the F1-scores is 0.1313, which is relatively high given the mean. This indicates that the model's performance can vary significantly depending on the specific data split. The high standard deviation suggests that while the model can perform very well in some scenarios (e.g., F1-score of 0.8752), it can also perform considerably worse in others (e.g., F1-score of 0.5006). This variability points to a potential lack of stability in its performance across different data subsets, which might warrant further investigation into the data distribution across folds or further hyperparameter tuning to improve robustness.

**Conclusion**: The tuned Random Forest model shows a good average F1-score, indicating reasonable overall performance. However, the relatively high standard deviation in F1-scores across cross-validation folds suggests that its performance is not perfectly consistent. This variability should be considered when assessing the model's reliability in a production environment.

##**6. Feature Importance Analysis**

We will dentify and visualize the most important features that contribute to the predictions of the best-performing model (Random Forest). This will provide valuable insights into the factors influencing insurance claims.
"""

# Extract feature importances from the best Random Forest model
feature_importances = pd.Series(best_rf_model.feature_importances_, index=X_train.columns)

# Create a DataFrame from the Series and sort it
importance_df = feature_importances.reset_index()
importance_df.columns = ['Feature', 'Importance']
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Feature importances extracted and sorted successfully.")
# Display the top features
display(importance_df.head(10))

"""We will visualize the top 10 most important features. We will use the `importance_df` and generate a horizontal bar plot to display these features.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the top 10 most important features
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')
plt.title('Top 10 Most Important Features in Random Forest Model')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.show()

print("Top 10 feature importances visualized successfully.")

"""##**7. Save the Best Model**

Now that the best model is identified and validated, we will save the trained model using techniques like joblib or pickle. This allows for easy deployment and reuse without retraining.

"""

import joblib

# Save the best_rf_model to a file
joblib.dump(best_rf_model, 'best_random_forest_model.joblib')

print("Best Random Forest model saved to 'best_random_forest_model.joblib'.")

"""Now we will provide a comprehensive summary of the entire project, including key insights from EDA, model performance, and feature importance. We will ensure the notebook is well-documented and ready for GitHub submission, as required by the project brief.

## Summary:

### Data Analysis Key Findings

*   **Optimal Random Forest Hyperparameters:** Through `GridSearchCV`, the best parameters for the Random Forest Classifier were identified as `n_estimators`: 300, `max_depth`: 30, `max_features`: 'sqrt', `min_samples_leaf`: 1, and `min_samples_split`: 5. This combination yielded the highest F1-score during the tuning process.
*   **GridSearchCV Performance:** The hyperparameter tuning achieved a best F1-score of approximately 0.8086 on the training data.
*   **Cross-Validation Performance:** The tuned Random Forest model demonstrated a mean F1-score of 0.7361 across 5-fold cross-validation. However, there was a significant standard deviation of 0.1313 in the F1-scores (ranging from 0.5006 to 0.8752), indicating variability in the model's performance across different data subsets.
*   **Key Feature Importances:** The most influential features in predicting insurance claims by the Random Forest model are `log_building_dimension` (0.1955), `building_dimension` (0.1947), `log_building_age` (0.1194), and `building_age` (0.0974). This highlights the critical role of building size and age in the model's predictions.
*   **Model Persistence:** The best-performing Random Forest model was successfully saved as `best_random_forest_model.joblib` for future deployment and use.

### Insights or Next Steps

*   **Address Model Instability:** The relatively high standard deviation in cross-validation F1-scores suggests that while the model has a good average performance, its consistency could be improved. Further investigation into data distribution, ensemble methods like stacking, or more robust cross-validation strategies could help mitigate this variability.
*   **Feature Engineering based on Importance:** Given the high importance of `building_dimension` and `building_age` (and their log transformations), exploring more advanced feature engineering based on these attributes (e.g., interaction terms, polynomial features) might yield further performance gains.
"""